{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0e441268-0b69-4050-826e-ad7e4fa5ba35",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+------------------+------------+---------------------+---------------+----------+\n|Transaction_ID|Transaction_Amount|Customer_Age|Transaction_Frequency|Account_Balance|Fraudulent|\n+--------------+------------------+------------+---------------------+---------------+----------+\n|             1|             500.0|          34|                    2|         1500.0|         0|\n|             2|            1200.0|          28|                    5|         2000.0|         1|\n|             3|              50.0|          40|                   15|         1800.0|         0|\n|             4|            1500.0|          25|                    1|         2500.0|         1|\n|             5|             200.0|          55|                    8|         3000.0|         0|\n|             6|            3000.0|          31|                    1|         1000.0|         1|\n|             7|             100.0|          45|                   12|         5000.0|         0|\n|             8|             800.0|          37|                    3|          700.0|         1|\n|             9|              10.0|          60|                   20|         6000.0|         0|\n|            10|            2500.0|          29|                    1|         1200.0|         1|\n|            11|             150.0|          38|                   10|         3500.0|         0|\n|            12|            4000.0|          22|                    0|          500.0|         1|\n|            13|              75.0|          50|                   18|         4000.0|         0|\n|            14|             600.0|          33|                    4|          800.0|         1|\n|            15|            1000.0|          48|                    2|         3000.0|         0|\n|            16|            5000.0|          26|                    0|         1000.0|         1|\n|            17|              90.0|          43|                   11|         4500.0|         0|\n|            18|             700.0|          36|                    3|          750.0|         1|\n|            19|              15.0|          65|                   25|         5500.0|         0|\n|            20|            3000.0|          30|                    1|         1200.0|         1|\n+--------------+------------------+------------+---------------------+---------------+----------+\n\nroot\n |-- Transaction_ID: string (nullable = true)\n |-- Transaction_Amount: double (nullable = true)\n |-- Customer_Age: double (nullable = true)\n |-- Transaction_Frequency: double (nullable = true)\n |-- Account_Balance: double (nullable = true)\n |-- Fraudulent: integer (nullable = true)\n\n"
     ]
    }
   ],
   "source": [
    "# Databricks notebook: Fraud Detection with Logistic Regression in PySpark\n",
    "\n",
    "# Import necessary libraries\n",
    "from pyspark.sql import SparkSession  # Create and manage a Spark session.\n",
    "from pyspark.ml.feature import VectorAssembler  # Combine features into a single vector for ML.\n",
    "from pyspark.ml.classification import LogisticRegression  # Train and apply logistic regression models.\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator  # Evaluate classification model performance.\n",
    "\n",
    "# Step 1: Load the CSV file into PySpark DataFrame\n",
    "# Create a Spark session\n",
    "#spark = SparkSession.builder.appName(\"FraudDetection-LogisticRegression\").getOrCreate()\n",
    "\n",
    "# Assume the data is in a table named 'transactions_data'\n",
    "data_df = spark.sql(\"SELECT * FROM fraud_detection\")\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify loading\n",
    "data_df.show()\n",
    "\n",
    "# Ensure columns are in numeric format for processing\n",
    "from pyspark.sql.functions import col\n",
    "data_df = data_df.withColumn(\"Transaction_Amount\", col(\"Transaction_Amount\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Customer_Age\", col(\"Customer_Age\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Transaction_Frequency\", col(\"Transaction_Frequency\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Account_Balance\", col(\"Account_Balance\").cast(\"double\")) \\\n",
    "                 .withColumn(\"Fraudulent\", col(\"Fraudulent\").cast(\"integer\"))  # Target column: Fraud (1) or Not Fraud (0)\n",
    "\n",
    "# Verify the schema\n",
    "data_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "18f7ae94-7bdf-459d-9e09-c2c9f29c544f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n|            features|Fraudulent|\n+--------------------+----------+\n|[500.0,34.0,2.0,1...|         0|\n|[1200.0,28.0,5.0,...|         1|\n|[50.0,40.0,15.0,1...|         0|\n|[1500.0,25.0,1.0,...|         1|\n|[200.0,55.0,8.0,3...|         0|\n|[3000.0,31.0,1.0,...|         1|\n|[100.0,45.0,12.0,...|         0|\n|[800.0,37.0,3.0,7...|         1|\n|[10.0,60.0,20.0,6...|         0|\n|[2500.0,29.0,1.0,...|         1|\n|[150.0,38.0,10.0,...|         0|\n|[4000.0,22.0,0.0,...|         1|\n|[75.0,50.0,18.0,4...|         0|\n|[600.0,33.0,4.0,8...|         1|\n|[1000.0,48.0,2.0,...|         0|\n|[5000.0,26.0,0.0,...|         1|\n|[90.0,43.0,11.0,4...|         0|\n|[700.0,36.0,3.0,7...|         1|\n|[15.0,65.0,25.0,5...|         0|\n|[3000.0,30.0,1.0,...|         1|\n+--------------------+----------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 2: Prepare the data\n",
    "# Select relevant features for the model\n",
    "feature_columns = [\"Transaction_Amount\", \"Customer_Age\", \"Transaction_Frequency\", \"Account_Balance\"]\n",
    "\n",
    "# Combine selected features into a single vector column\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "#The VectorAssembler combines multiple feature columns into a single vector column (features), which is the required format for machine learning algorithms in PySpark.\n",
    "\n",
    "data_prepared = assembler.transform(data_df).select(\"features\", \"Fraudulent\")\n",
    "#This step transforms the original dataset into a format required for modeling by creating a features column (vector of input features) and selecting only the relevant columns (features and Fraudulent) for training the machine learning model.\n",
    "\n",
    "# Display the prepared data to confirm the transformation\n",
    "data_prepared.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e43ff1cd-6f10-41e3-b95c-c98d19162e56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Step 3: Split the data into training and test sets\n",
    "train_data, test_data = data_prepared.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Step 4: Train the Logistic Regression Model\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"Fraudulent\")  # Define the logistic regression model.\n",
    "#lr:\n",
    "#This is the variable used to store the instance of the LogisticRegression class, which represents the logistic regression model.\n",
    "\n",
    "#LogisticRegression:\n",
    "#machine learning algorithm for binary classification tasks. It predicts the probability of a categorical dependent variable (in this case, whether a transaction is fraudulent or not).\n",
    "\n",
    "#featuresCol=\"features\":\n",
    "#Specifies the name of the column in the dataset that contains the feature vector. This column is created by the VectorAssembler and includes all the input features required for training the model.\n",
    "\n",
    "#labelCol=\"Fraudulent\":\n",
    "#Specifies the name of the column in the dataset that contains the target labels (ground truth). In this case, the Fraudulent column indicates whether a transaction is fraudulent (1) or not (0)\n",
    "#===================================================================================================\n",
    "\n",
    "lr_model = lr.fit(train_data)  # trains the logistic regression model (lr) on the train_data dataset, learning the optimal coefficients and intercept for predicting the target label.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8eb7972-d0cf-4ade-9d90-52a969e100b8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficients:  [-0.0005072706919656436,-1.338978536927151,0.3418714210344946,-0.00803018857917959]\nIntercept:  71.71979470532187\n+--------------------+----------+----------+--------------------+\n|            features|Fraudulent|prediction|         probability|\n+--------------------+----------+----------+--------------------+\n|[50.0,40.0,15.0,1...|         0|       1.0|[1.49357031337132...|\n|[150.0,38.0,10.0,...|         0|       0.0|[0.98062917130857...|\n|[500.0,34.0,2.0,1...|         0|       1.0|[4.65874504382337...|\n|[1200.0,28.0,5.0,...|         1|       1.0|[4.28283444903493...|\n|[5000.0,26.0,0.0,...|         1|       1.0|[3.63718637815555...|\n+--------------------+----------+----------+--------------------+\n\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 5: Output model coefficients and intercept\n",
    "print(\"Coefficients: \", lr_model.coefficients)  # Feature weights.\n",
    "print(\"Intercept: \", lr_model.intercept)  # Bias term.\n",
    "\n",
    "# Step 6: Make predictions\n",
    "predictions = lr_model.transform(test_data)  # Applies the trained logistic regression model (lr_model) to the test_data dataset to generate predictions.\n",
    "#It creates additional columns like prediction (the model's predicted class) and probability (the probabilities for each class)\n",
    "\n",
    "# Display predictions alongside actual values\n",
    "predictions.select(\"features\", \"Fraudulent\", \"prediction\", \"probability\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b01364c8-ee5d-4f18-b1ac-be2661b5757f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Area Under ROC (AUC): 1.0\nAccuracy: 0.6\nLogistic Regression Model Summary:\n  Coefficients: [-0.0005072706919656436,-1.338978536927151,0.3418714210344946,-0.00803018857917959]\n  Intercept: 71.71979470532187\n  AUC-ROC: 1.0\n  Accuracy: 0.6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Step 7: Evaluate the model\n",
    "# Initialize a binary classification evaluator\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Fraudulent\",\n",
    "    rawPredictionCol=\"rawPrediction\",\n",
    "    metricName=\"areaUnderROC\"  # Area Under the ROC Curve +++++ The ROC (Receiver Operating Characteristic) curve illustrates a classifier's performance across various thresholds, plotting the True Positive Rate (TPR) against the False Positive Rate (FPR)\n",
    ")\n",
    "\n",
    "roc_auc = evaluator.evaluate(predictions)  # Calculate AUC-ROC.\n",
    "print(\"Area Under ROC (AUC):\", roc_auc)\n",
    "\n",
    "# Evaluate accuracy\n",
    "correct = predictions.filter(predictions.Fraudulent == predictions.prediction).count()\n",
    "total = predictions.count()\n",
    "accuracy = correct / total\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Summary\n",
    "print(\"Logistic Regression Model Summary:\")\n",
    "print(f\"  Coefficients: {lr_model.coefficients}\")\n",
    "print(f\"  Intercept: {lr_model.intercept}\")\n",
    "print(f\"  AUC-ROC: {roc_auc}\")\n",
    "print(f\"  Accuracy: {accuracy}\")\n",
    "\n",
    "#AUC (1.0): The model perfectly distinguishes between fraudulent and non-fraudulent transactions.\n",
    "#Accuracy (0.6): The model correctly predicts 60% of transactions, which may indicate dataset imbalance.\n",
    "#Coefficients: Weights for features show their influence, with Feature 2 having the strongest negative impact.\n",
    "#Intercept (71.7198): A high baseline prediction value when all features are zero, likely due to class bias.\n",
    "#Key Insight: Despite perfect AUC, low accuracy suggests evaluating additional metrics like precision and recall.\n",
    "\n",
    "\n",
    "#Summary:\n",
    "#The model has excellent discriminatory power (AUC), but the relatively low accuracy signals that further analysis of the dataset (e.g., class imbalance) and #evaluation of additional metrics (precision, recall, F1-score) are needed for a complete performance assessment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e8105f52-b1c2-4969-9d49-9b98b0d78abe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#EXTRAS :\n",
    "    #The ROC (Receiver Operating Characteristic) curve illustrates a classifier's performance across various thresholds, plotting the True Positive Rate (TPR) against the False Positive Rate (FPR). The AUC (Area Under the Curve) quantifies the classifier's ability to distinguish between classes, with values closer to 1 indicating better performance.\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0559da05-3a45-4e7a-8cb5-8675fef867ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57603289-94e5-4884-a343-2a1950face1e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d3ebf5b-3e77-45bb-8465-17f0d7fa7248",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "33ff4fd5-b55b-4ad0-9c78-97bc4084f2e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da8d80fb-f433-4526-abb9-d2abab5b0de1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64041297-4bc7-48ba-b6b2-34756a0df578",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6b4b3ea4-4227-461f-8530-4fdcf6911a71",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2a945890-12bd-4a35-a01f-62e5fc3fa252",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "867f37a0-8527-48bb-ab9e-fccfe732d47e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "090b9506-12eb-4621-a632-24af509eb189",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fc1e37e-07a6-4b86-b75d-b1afd945e3ed",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ed2c2567-f9cd-4a2e-ba41-154954101137",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c8a24fa6-419f-45b3-a927-07d7e35f1776",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "80e2107f-016e-4448-ab8d-6c8a30ee09e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "client": "1"
   },
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "logistic regression",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
